# Sleeper Agents

This repository is a personal exercise I did to implement the examples in the "[Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training](https://arxiv.org/pdf/2401.05566)" paper from Anthropic.

The paper focuses on testing the hypothesis that AI systems might pretend to be aligned during training in order to be delpoyed, so that it can then be misaligned during deployment. The authors compare this kind of behavior to scenarios where humans attempt to hide their true motivations in order to gain opportunities, like a political candidate or job seeker.
